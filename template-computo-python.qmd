# Introduction

## Bayesian workflow

@gelman2013bda begin their foundational textbook by factoring Bayesian data analysis into three steps.

1. Design a joint probability distribution for observable data and unobservable parameters.
2. Perform inference to generate a posterior sample over parameters conditioned on data.
3. Evaluate the model fit and what it tells us about our quantities of interest.

The authors further suggest that if the evaluation in step (3) is not sufficient, then one should go back to step (1) and try to come up with a better model.  More recently, @gelman2020workflow outlined a workflow for Bayesian analysis that puts more emphasis on evaluating multiple models simultaneously and transparently reporting their exploration and comparison.

A probabilistic programming language (PPL) primarily provides support for coding statistical models defined in step (1) in a way that they can be used for inference in step (2).  PPLs are typically coupled with samplers that allow step (2) to be performed with Monte Carlo methods.  PPLs are typically coupled with posterior analysis and model comparison tools for task (3).

In this paper, we are going to focus on performing the same function as a PPL without actually using a PPL.  We will also provide guidance on integration with tools for inference, model evaluation/criticism, and model comparison.  Our goal is to develop a methodology for implementing efficient and scalable differentiable Bayesian models in Python in a way that is both easy to code and easy to read.  

For step (2), we recommend the Python package Blackjax [@cabezas2024blackjax]. Blackjax includes implementations of Stan's primary inference methods, the no-U-turn sampler (NUTS) [@hoffman2014no], automatic differentiation variational inference (ADVI) [@kucukelbir2017automatic], and Pathfinder variational inference [@zhang2022pathfinder].  Blackjax is being actively maintained and extended, and already includes several other useful algorithms, including microcanonical (aka isokinetic) sampling [@robnik2025metropolis], sequential Monte Carlo (SMC) [@doucet2001introduction], elliptical slice sampling [@murray2010elliptical], generalized HMC (GHMC) [@horowitz1991generalized], and even random-walk Metropolis (RWM) [@hastings1970monte].

For step (3), we recommend the Python package Arviz [@kumar2019arviz].  Arviz provides state-of-the-art convergence monitoring using split, ranked $\widehat{R}$, estimation of bulk and tail effective sample sizes, standard errors, posterior means, standard deviations, and quantiles, as well as approximate leave-one-out (LOO) cross-validation [@vehtari2021rank; @vehtari2017practical].

## Why not just use Stan?

Stan [@carpenter2017stan] is a domain-specific language for expressing differentiable probability densities and posterior predictive quantities.  Stan is a probabilistic programming language in the sense that its variables can be interpreted as random variables.  Stan has been used in almost every area where statistics is applied, and as such, has accumulated an unmatched depth and breadth of training materials around different classes of probabilistic models.  There are textbooks, college classes, and reproducible case studies in all of these areas.  There's a vibrant community with a high volume [discussion forum](https://discourse.mc-stan.org).  The language is still being expanded and so is it's math library.

The Stan project introduced several state of the art gradient-based inference algorithms including the no-U-turn sampler (NUTS) [@hoffman2014no], automatic differentiation variational inference (ADVI) [@kucukelbir2017automatic], Pathfinder variational inference [@zhang2022pathfinder], and black-box nested Laplace approximations [@margossian2023general] as well as posterior analysis tools such as split- and ranked-$\widehat{R}$ and corresponding bulk and tail effective sample size [@vehtari2021rank], leave-one-out cross-validation [@vehtari2017practical], refined simulation-based calibration checks [@talts2018validating], and prior predictive checks [@gabry2019visualization].  It is often used as the basis of methodological developments such as Bayesian workflow [@gelman2020workflow].
 
So why not just use Stan? The first reason is that Stan is largely CPU-bound.  All of its analysis tools and algorithms run on the CPU.  Although there are ways to call individual functions in a Stan program on the GPU (e.g., Cholesky decomposition) and ways to apply map-reduce across multiple cores, this is not enough.  Stan lacks a way to keep computation in-kernel on the GPU or organize inference to enable single-instruction multiple-data (SIMD) parallelism.  As a result, Stan is not competitive on modern hardware [@sountsov2024running; @maskell2024jaxgpu].

The second obstacle to using Stan is the need to learn a second language. While Stan is not particularly complicated, it does present several additional difficulties beyond unfamiliar syntax and semantics.

* Stan is indexed from 1, like much of mathematics and in particular, linear algebra, whereas Python is indexed from 0, like most programming languages.  Translating between 0-based and 1-based indexing is tedious, error-prone, and obfuscates code.
* Stan is strongly typed and statically compiled, which has the benefit of being type-safe at run time and leads to fast C++ computation.  The downside is that this kind of static typing is unfamiliar to most of Stan's intended users.  Stan's typing can be an annoyance and performance bottleneck even for experienced programmers due to its poor representational choice for containers that mixes C++ standard vectors [@josuttis2012cpp] for arrays and Eigen matrices [@guennebaud2010eigen] for linear algebra (this may sound harsh, but it was our fault as the original developers of Stan).
* Stan requires a scripting language like R, Python, and Julia, in which to run rather than providing a seamless single-language experience.  The interface between these languages and Stan is minimal---Stan is just being called as a black box to return samples.
* There is relatively little tooling to aid with Stan development.  Currently, it's just autocomplete, syntax highlighting, and debug-by-print.  Python, in contrast, has several well supported integrated development environments with tooling for debugging, generating notebooks, integration with chatbots, integration with documentation, automatic refactoring tools, etc.
* While there is a great deal of tutorial and onboarding material for Stan, it has to split its attention among several interfaces (two interfaces in R and Python and one in Julia).  Taken together, even Stan's extensive documentation is dwarfed by the pedagogical material around scientific, differentiable, and probabilistic computing in Python.  


## Why not just use PyMC or NumPyro?

The very first probabilistic programming language of which we are aware is BUGS (Bayesian inference using Gibbs Sampling) [@lunn2009bugs; @lunn2012bugs], which was released way back in 1991.  In BUGS, Bayesian models are specified with deterministic and stochastic nodes arranged in a directed acyclic graph.  Each node was either input as data, or defined as a (possibly stochastic) function of its direct ancestors in the graph.  This enabled a BUGS model to be used to infer any of the stochastic variables in the model given values for the data nodes and all other stochastic nodes.  This provides a clean way to perform analyses such as prior predictive inference and posterior predictive inference automatically through the graphical structure of the model.  BUGS samples using generalized Gibbs sampling, which does not scale well in dimension.

PyMC [@salvatier2016probabilistic] and NumPyro [@phan2019composable] are Python packages that take a directed graphical modeling approach to specifying Bayesian models and are capable of generating JAX code as output.  There are similar packages in other languages, but they do not generate JAX code.  JAGS [@plummer2003jags] is a standalone language that reimplements and extends BUGS and is typically used through R, NIMBLE [@de2017programming] is coded in R, and Turing.jl [@ge2018turing] is coded in Julia.

Like Stan, all of the modern PPLs efficiently scale in dimension by using gradient-based inference methods.  Like BUGS, they are able to exploit the graphical model structure directly to automate a number of functions that are painful to code in Stan and will largely remain painful to code in what we are proposing here, such as prior and posterior predictive checks and simulation-based calibration, and at least in the case of PyMC, general patterns of missing data.

When models get more complicated in terms of novel parameter constraints, densities, conditional structures, etc., both PyMC and NumPyro provide escape hatches to let you define transformations and log densities directly in the same way as Stan. This feels dirty in the same way as using "ones-trick" in BUGS [@lunn2012bugs]. And while it is great for allowing general models to be defined, it defeats all the benefits of having a clean generative graphical model in the first place.  At the point models start getting more complicated, we believe it is more straightforward to code the models directly in JAX rather than working around the graphical modeling paradigm of PyMC or NumPyro.

A second reason to prefer the approach we are presenting here is that it is much more direct.  By that, we mean that like Stan, the resulting code is implemented transparently in an imperative fashion rather than indirectly through the structure of the directed acyclic graph.

## Special function support

Stan has an extensive library of special mathematical and statistical functions, as well as restructuring functions for arrays and matrices.  Many of these are needed to differentiate cumulative distribution functions and to define custom densities.  Here's a brief overview of the coverage available in JAX compared to Stan.  The bottom line is that support for special functions is much much better in JAX than in Stan.

* *Matrix library*:  This is JAX's main focus and it far exceeds Stan's collection of familiar matrix functions and reshaping tools by punning NumPy ([`jax.numpy`](https://docs.jax.dev/en/latest/jax.numpy.html)) and SciPy ([`jax.scipy`](https://docs.jax.dev/en/latest/jax.scipy.html)).  There is even limited (and experimental) support for sparse matrices and solvers natively ([`jax.experimental.sparse`](https://docs.jax.dev/en/latest/jax.experimental.sparse.html)).

* *Special functions*:  These are available all over the JAX ecosystem, including in JAX's NumPy and SciPy modules.  The differentiable SciPy module is not complete compared to SciPy.  The deficit is more than made up by the special function library provided by TensorFlow Probability (TFP) ([`jax.scipy.special`](https://docs.jax.dev/en/latest/jax.scipy.html#module-jax.scipy.special)) and maintained by Google.  For example, the Lambert W function available in Stan has not been ported from SciPy but is available through TFP.  The bottom line is that JAX provides a *better* selection of well supported special functions.

* *Probability distributions*:  Stan implements dozens of probability distributions, including almost most (but not all) of the ones in common use for statistical models.  While the basics are available through JAX's NumPy and SciPy modules (often redundantly), the go-to library TFP ([`tfp.probability`](https://www.tensorflow.org/probability?hl=en)) for probability-related functions (e.g., probability density functions, probability mass functions, and cumulative distribution functions).  In some cases, there are also quantile functions, which are poorly supported in Stan.  The bottom line is that the native Google-supported *JAX* ecosystem provides a *better* selection of well supported probability functions and random number generators.  There is even wider support beyond native JAX and TensorFlow, including the probabilistic programming language NumPyro [@phan2019composable] and Google DeepMind's library Distrax [@deepmind2020jax].

* *Complex-valued functions*: JAX and Stan both have core library support for complex-valued functions built-in, including fast Fourier transforms and complex matrix operations.

* *Neural networks*: JAX is integrated tightly with a range of neural network constructions through the Flax package [@heek2020github], which is maintained by a team at Google DeepMind, but is not an official Google product like TensorFlow or JAX.  It supports everything from simple multilayer perceptrons and convolutional neural networks to autoencoders and multi-head attention.

* *Simulation-based inference (SBI)*: There is as of yet, no mature and commonly used SBI package in JAX, nor is there any support in Stan.
    
* *Implicit Solvers*:  Applied statistics often requires equations to be solved and differentiated and Stan provides a fairly extensive library.
    * *Ordinary differential equation (ODE) solvers*: There is no built-in support in JAX for ODE solvers, but the Diffrax package [@kidger2021on] is widely used and provides the same kind of adjoint and analytic methods as Stan that provide sensitivity analysis without automatically differentiating through the algorithm.  

    * *Root finders*:  There is no built-in support in JAX for root finders, but the JAXOpt package [@blondel2021jaxopt] is maintained by Google and provides a range of solvers including the Newton method used by Stan.

    * *1D Integration*: These functions are useful for defining cumulative distribution functions for novel densities. There is no built-in or widely used external support for 1D integration in JAX at the moment. This is the only problem for which Stan has a solution, but JAX has none.
    
    * *Hidden Markov models*:  TFP (through [`tfp.distributions.HiddenMarkovModel`](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/HiddenMarkovModel?hl=en)) provides support for the forward algorithm to compute the distribution of outcomes given parameters (i.e., implement a likelihood function). This is enough to fit models out of the box, but it is not as efficient as the full forward-backward algorithm in Stan.
    
    * *Kalman filters*:  There is no direct support in Stan; there is experimental support in TFP (in [`tfp.experimental.parallel_filter.kalman_filter`](https://www.tensorflow.org/probability/api_docs/python/tfp/experimental/parallel_filter/kalman_filter?hl=en)).
    
    * *Partial differential equation (PDE) solvers*:   Stan has no support for partial differential equations.  The JAX-CFD package [@kochkov2021cfd] from Google, mostly focused on fluid dynamics, includes a number of general-purpose PDE solvers, including finite volume/difference methods, pseudospectral methods, and machine-learning methods.  It supports Navier-Stokes, advection-diffusion, period and wall boundary conditions, and runs on GPU and TPU with gradients.

    * *Stochastic differential equation (SDE) solvers*:  Stan has no support for stochastic differential equations. The Diffrax package [@kidger2021on], in addition to providing stiff ODE solvers, also supports traditional SDE and SPDE solvers (Eueler-Maruyama, Milstein, Stratonovich/Itô) through spatial discretization.



Other than the omission of 1D integration and the backward pass of the forward-backward algorithm, the special function and probability distribution and transform libraries of JAX are *better* than those supplied by Stan.  When it comes to even more complicated functions like PDE and SDE solvers and neural networks, Stan isn't even in the game.  The JAX-based PPL NumPyro can also make use of these JAX packages in many cases, especially if users are willing to forego its underlying graphical model abstraction.


## Constrained parameter support

Stan provides built-in transforms for constrained parameters to provide densities with support over all of $\mathbb{R}^D$, along with the corresponding change-of-variables adjustments.  These are custom implementations with analytic Jacobian-adjoint product gradients and vectorized application to containers.

The Oryx transform library in TensorFlow is built on top of the TFP bijector library ([`tfp.bijectors`](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors)) [@dillon2017tensorflow], which can also be used directly.  TFP bijectors provide all of the transforms provided by Stan and many more including softplus, various cdfs and hyperbolic tangent as replacements for inverse-logit, more multivariate transforms such as cumulative sums and Householder factorizations, as well as trained transforms like RealNVP normalizing flows [@dinh2016density].  Oryx additionally allows transforms to be written down directly in such a way that Oryx can automatically calculate inverse transforms and Jacobian determinants of inverse transforms.

## Modularity

SlicStan [@gorinova2019slic] reconceived Stan without blocks---the sorting into data, parameters, and generated quantities was carried out by data flow analysis.  The primary motivation was to make it possible to modularly express concepts like a hierarchical prior.  With Stan itself, this is impossible unless the modularity is in the form of a simple function.  With SlicStan, the parameters, priors, etc., could all be constructed modularly and reused.  By allowing models to be expressed directly in Python code, NumPyro and PyMC already support modular code reuse.  Although it is rare to see this feature used in example code, it is widely used in production.

By coding models directly in Python, we gain the same benefits of NumPyro and PyMC.  We can write general programs returning arbitrary components of a probabilistic program and combine them at will.  We will provide examples below.


# The Bayesian inference problem

In this section, we will lay out the precise definition of the Bayesian inference problem we are trying to compute, show how it can be computed asymptotically exactly using Monte Carlo methods, and apply some simple calculus to transform parameterizations to be unconstrained (i.e., have support over all of $\mathbb{R}^D$).  

## Bayesian models and inference

A Bayesian model is nothing more than a joint distribution over parameters $\theta$ and data $y$.  By the chain rule, the posterior distribution of the parameters given the data is given by $p(\theta \mid y) \propto p(\theta, y)$.  Often our density is factored into a prior times a likelihood, $p(\theta \mid y) = p(\theta) \cdot p(y \mid \theta)$, but nothing in Stan or what we are proposing for JAX presupposes a clean factorization.  In fact, our methods can be used to sample from any old density---it doesn't have to arise as a Bayesian posterior.

For Bayesian inference, we are typically interested in expectations with respect to the posterior.  For instance, the parameter estimate that minimizes expected square error (assuming the model is correct) is the posterior mean,
$$
\widehat{\theta} = \mathbb{E}[\theta \mid y].
$$
The posterior covariance is
$$
\textrm{var}[\theta] = \mathbb{E}[(\theta - \widehat{\theta}) \cdot (\theta - \widehat{\theta})^\top].
$$
If we have an event $A \subseteq \mathbb{R}^D$, then its probability is given by the expectation of its indicator function,
$$
\Pr[\theta \in A] = \mathbb{E}[\mathbb{1}_A(\theta \in A) \mid y].
$$
If we want to evaluate posterior predictive densities for new data $\tilde{y}$, then
$$
p(\tilde{y} \mid y) = \mathbb{E}[p(\tilde{y} \mid \theta) \mid y].
$$

Conditional expectation notation is just shorthand for a high-dimensional integral in our underlying probability space,
$$
\mathbb{E}[f(\theta) \mid y]
= \int_{\mathbb{R}^D} f(\theta) \cdot p(\theta \mid y) \, \textrm{d}\theta.
$$
We can reduce the high-dimensional integration problem to one of sampling from the posterior,
$$
\theta^{(m)} \sim p(\theta \mid y).
$$
Given modest assumptions to ensure ergodicity [@roberts2004mcmc], the Monte Carlo estimate converges to the true value,
$$
\mathbb{E}[f(\theta) \mid y]
= \lim_{M \rightarrow \infty} \
  \frac{1}{M} \sum_{m < M} f(\theta^{(m)}).
$$
In practice, we use a finite approximation for a fixed $M$,
$$
\mathbb{E}[f(\theta) \mid y]
\approx  \frac{1}{M} \sum_{m < M} f(\theta^{(m)}).
$$
If the Markov chain is well behaved (e.g., it's geometrically ergodic), then the Markov chain central limit theorem will hold, and the Monte Carlo estimate will converge to the true value at a rate of $\mathcal{O}(1 / \sqrt{M})$ [@roberts2004mcmc].

## Unconstrained parameterizations

It is much easier to define a sampling algorithm for situations where the posterior has support over all of $\mathbb{R}^D$, i.e., for all $\theta \in \mathbb{R}^D$, $p(\theta \mid y) > 0$.  Stan and other PPLs transform any unconstrained parameters to be unconstrained.  Then in practice, the unconstrained parameters are inverse transformed to satisfy their constraints.  This requires an adjustment for the change of variables, which happens implicitly in PPLs.  Stan requires all parameter constraints to be declared; the graphical modeling sublanguage of PyMC and NumPyro infer these constraints from the distributions in which variables participate (e.g., if a variable is given a Wishart distribution, it must be a symmetric and positive definite matrix).

Mathematically, given a constrained random variable $\Theta \in C \subseteq \mathbb{R}^D$, with a density $p_\Theta(\theta)$, and a smooth bijection $f:C \rightarrow \mathbb{R}^N$, we can derive the density of $\Phi = f(\Theta)$ as
$$
p_\Phi(\phi) = p_\Theta(f^{-1}(\phi)) \cdot \left| \nabla f^{-1}(\phi) \right|,
$$
where $|\cdot|$ denotes the absolute determinant operator and $f^{-1}$ is the inverse of $f$.  In the univariate case, $\nabla f^{-1}(y)$ reduces to the derivative of the inverse transform at $y$ (i.e., $\nabla f^{-1} = (f^{-1})'$).

If there is a sequence of variables being transformed one at a time, the overall Jacobian will be block diagonal, with an absolute Jacobian determinant equal to the product of the absolute Jacobian determinants of the blocks.  Unconstrained parameters are transformed by the identity, which has a unit Jacobian determinant.  This makes it particularly simple to work on the unconstrained scale---we just map unconstrained parameters back to the constrained space using the inverse transforms and add the log absolute determinants of their Jacobians.  For maximum likelihood estimation, the Jacobians can be dropped from the target density with a flag.

Stan supplies constraints for variables that are lower bounded (for scales), upper bounded (for log probabilities), range bounded (for probabilities), affine transforms (for non-centered parameterizations), ordered vectors (for cutpoints in ordinal regressions or identifying mixtures), unit vectors (for points on a hypersphere), simplexes (for categorical probability distributions), sum-to-zero vectors (for identifying varying effects), positive-definite symmetric matrices and their Cholesky factors (for covariance or precision matrices), and for unit-diagonal positive-definite matrices and their Cholesky factors (for correlation matrices). The Oryx package, which is native to JAX and built on top of the TensorFlow Probability bijectors package [@dillon2017tensorflow], provides an even wider range of useful transforms than Stan (e.g., softplus, alternative sigmoid cdfs, tanh, autoregressions, and flows, and many many more).


# A worked example

To ground the discussion, let's consider the concrete example of coding a linear regression and using it to predict new observations.

## A multivariate linear regression model

We will assume a very simple multivariate regression formulation with an intercept, $P \in \mathbb{N}$ covariates, and $N \in \mathbb{N}$ observations.  Our data is made up of observations $y_n \in \mathbb{R}$ paired with covariates $x_n \in \mathbb{R}^{N \times P}$.  We will assume the usual parameters consisting of a slope $\alpha \in \mathbb{R}$, regression coefficients $\beta \in \mathbb{R}^P$, and an error scale $\sigma \in (0, \infty)$.  We will assume the data and covariates are unit scale so that we can assume a weakly informative prior independently over our paraemters,
$$
\alpha \sim \textrm{normal}(0, 5)
\qquad
\beta_p \sim \textrm{normal}(0, 2.5)
\qquad
\sigma \sim \textrm{exponential}(0.5).
$$
We then add the conventional data generating process with independent normal errors, 
$$
y_n \sim \textrm{normal}(\alpha + x_n \cdot \beta, \sigma).
$$
The joint density defining our Bayesian model (with data $x$ taken as an unmodeled constant) is thus
$$\textstyle
\begin{array}{rcl}
p(y, \alpha, \beta, \sigma \mid x)
& = &
  \textrm{exponential}(\sigma \mid 0.5)
  \cdot \textrm{normal}(\alpha \mid 0, 5)
\\[4pt]
& & {} \cdot \left( \prod_{p=1}^P \textrm{normal}(\beta_p \mid 0, 2.5) \right)
  \cdot \left( \prod_{n=1}^N \textrm{normal}(y_n \mid \alpha + x_n \cdot \beta, \sigma) \right).
\end{array}
$$
Bayes's rule allows us to use the joint density as an unnormalized posterior,
$$
p(\alpha, \beta, \sigma \mid y, x)
\propto p(y, \alpha, \beta, \sigma \mid x).
$$

The only constrained parameter is $\sigma > 0$.  We transform positive-constrained parameters using the log transform, e.g., $\sigma^\textrm{unc} = \log \sigma$. The inverse transform is the exponential. Applying the change-of-variables formula and using the fact that $\left| \nabla \exp(u) \right| = |\exp'(u)| = \exp(u)$, the corresponding unconstrained density is
$$
p^\textrm{unc}(\alpha, \beta, \sigma^\textrm{unc} \mid x, y)
= p(\alpha, \beta, \exp(\sigma^\textrm{unc}) \mid x, y) \cdot \exp(\sigma^\textrm{unc}).
$$
On the log scale where we operate to prevent underflow and maintain precision, we have
$$
\log p(\alpha, \beta, \sigma^\textrm{unc} \mid x, y)
= \log p(\alpha, \beta, \exp(\sigma^\textrm{unc}) \mid x, y) + \sigma^\textrm{unc}.
$$


## Linear regression in Stan

Here's an example Stan program defining a linear regression, which we have placed into the file `linear-regression.stan`. 

```stan
data {
  int<lower=0> N, N_new, P;
  matrix[N, P] x;
  vector[N] y;
  matrix[N_new, P] x_new;
}
parameters {
  real alpha;
  vector[P] beta;
  real<lower=0> sigma;
}
model {
  alpha ~ normal(0, 5);
  beta ~ normal(0, 2.5);
  sigma ~ exponential(0.5);
  y ~ normal(alpha + x * beta, sigma);
}
generated quantities {
  array[N_new] real y_new = normal_rng(alpha + x_new * beta, sigma);
}
```

To ease the transition to JAX, note that the distribution statements in the model block are just syntactic sugar for incrementing the log target density [@carpenter2017stan].  The model block could have been coded as follows to generate the same C++ code.

```stan
  target += normal_lupdf(alpha | 0, 5);
  target += normal_lupdf(beta | 0, 2.5);
  target += lognormal_lupdf(sigma | 0, 1);
  target += normal_lupdf(alpha + x * beta, sigma);
```

Here, the `_lupdf` indicates a log (`l`), unnormalized (`u`), probability density function (`pdf`).  For probability mass functions, replace `pdf` with `pmf`; to preserve normalizing constants, drop the `u`.

## Compilation to C++

A Stan program translates almost line-for-line to a C++ class that implements all the interfaces around a model required for log density evaluation, posterior predictive quantity generation, and variable transforms [@stan2025ref].  Going block by block,

* *Functions block*: Each function in the functions block is translated to a C++ function that is templated flexibly enough to allow automatic differentiation.  Forward declarations are made to support recursive definitions.

* *Data and transformed data blocks*:  Each data declaration in the data block is translated to an argument of the class constructor.    The transformed data block is executed after the data is read in as the model object is being constructed.  After the model object is constructed with the data, it remains immutable to allow safe multi-threaded application.

* *Parameter and transformed parameter blocks*:  The log density function is defined for unconstrained inputs gathered into a vector.  The frist part of the function unpacks the entries of that vector, applies the constraining transform to define constrained variables locally, then adds the log Jacobian determinant to the Jacobian accumulator to adjust for the change-of-variables making up the constraint.  Transformed  parameters also define local variables.  Everything executes serially as coded in the Stan program.

* *Model block*: The rest of the body of the log density function is the line-by-line translation of Stan's model block.  Each distribution and target increment statement increments the log density accumulator, which is returned as the value of the function.  Everything is templated generally enough and coded in the underlying math library to allow automatic differentiation.

* *Generated quantities block*: The generated quantities block translates to a function that maps the parameters and a random number generator to the variables of interest.  These are gathered by this function along with the parameters and transformed parameters to form vector output.  The names of all the parameters define the columns of output and output is on the constrained scale where the user and Stan program model block operate.

The data variables are specified in the `data` block in the Stan program.  Here, we have the sizes, the covariate matrices (`x` plus `x_new` for posterior prediction), and the outcomes (`y`).  The constructed C++ class is immutable and provides several methods, the most central of which is an unconstrained (in the sense of having support over all of $\mathbb{R}^D$) log density function that is templated in order to support automatic differentiation [@carpenter2015ad].  In math, the constraining transform maps $(\alpha, \beta, \sigma^\text{u})$ to $(\alpha, \beta, \exp(\sigma^\text{u}))$.  The transforms are independent and the first two are the identity, so the Jacobian determinant works out to $\exp(\sigma^\text{u})$.  Thus the additive change-of-variables adjustment on the log scale is just $\log \exp(\sigma^\text{u}) = \sigma^\text{u}$.

The model block defines a density function $p()$ over constrained parameters---this is typically derived and coded to implement the unnormalized posterior log density of a Bayesian model.  Densities are read elementwise, with scalar arguments being broadcast where necessary.  With this translation, the unnormalized log posterior over the constrained variables defined by the Stan program's model block is
$$
\log p(\alpha, \beta, \sigma \mid x, y)
 =  \log \textrm{normal}(\alpha \mid 0, 5)
 +  \sum_{p=1}^P \textrm{normal}(\beta_p \mid 0, 2.5) \\
 +  \log \textrm{gamma}(\sigma \mid 0.5) 
 +  \sum_{n=1}^N \textrm{normal}(y_n \mid \alpha + \beta \cdot x_n, \sigma).
$$
The corresponding unconstrained log density $q()$ over which inference is performed, adds the log Jacobian adjustment for the change of variables,
$$
\log q(\alpha, \beta, \sigma^\text{u} \mid x, y)
= \log p(\alpha, \beta, \exp(\sigma^\text{u}) \mid x, y) + \sigma^\text{u}.
$$

To support the changes of variables for reporting constrained output, the compiled C++ code exposes the constraining transform and its Jacobians. For initialization from constrained parameters, there is a matching unconstraining transform.  The model class also supplies methods for determining the shape and names of the constrained and unconstrained parameters.  

The final component of a compiled Stan model is a function to perform predictive inference as defined by the generated quantities block (this can also be done post-hoc with a new program and generated quantities block).  In particular, the model compiles a generated quantities function that takes a random number generator and produces the output defined in the generated quantities block purely by forward sampling without the need for any automatic differentiation.

## Running the Stan program

In order to fit a model, we need data.

### Simulating data

The sizes and covariates $x$ cannot be simulated from the model as they are not modeled.
Let's simulate some data.

```{python python-code}
#| code-fold: false
import json
import numpy as np

def simulate_regression(n=128, p=2, n_new=4, seed=145777):
    def simulate_covariates(n):
      x = rng.normal(size=(n, p))
      x[:, 0] = x[:, 0]**2
      return x

    rng = np.random.default_rng(seed)
    alpha = rng.normal(0.0, 5.0)
    beta = rng.normal(0.0, 2.5, size=p)
    sigma = rng.exponential(1.0 / 0.5)
    x = simulate_covariates(n)
    mu = alpha + x @ beta
    y = rng.normal(mu, sigma)
    x_new = simulate_covariates(n_new)
    parameters = { "alpha": alpha, "beta": beta, "sigma": sigma }

    data = {
        "N": n, "P": p, "N_new": n_new, "x": x.tolist(),
	"y": y.tolist(), "x_new": x_new.tolist(),
    }
    return parameters, data

params, data = simulate_regression()
```

The simulated parameter values are as follows.

```{python python-code}
#| code-fold: true
print(f"alpha:{params['alpha']:6.2f};   beta[0]:{params['beta'][0]:6.2f};   beta[1]:{params['beta'][1]:6.2f};   sigma:{params['sigma']:6.2f}")
```

It's always a good idea to plot your data when possible.  We provide a plot of training data $y$ as a function of $x$ in @fig-sim-3d.

```{python python-code}
#| label: fig-sim-3d
#| code-fold: true
#| fig-cap: "*Simulated data from the quadratic–linear regression model. The horizontal axes show the covariates `x` and the vertical axis the value `y`.*"

import plotly.express as px

def plot_3d(data):
    x = np.asarray(data["x"])
    y = np.asarray(data["y"])
    fig = px.scatter_3d(
        x=x[:, 0], y=x[:, 1], z=y,
        labels={"x": "x[1]", "y": "x[2]", "z": "y"},
    )
    return fig

sim_fig = plot_3d(data)
sim_fig
```



## Fitting the model with MCMC

Now that we have defined a Stan program and simulated data, we can perform inference based on the simulated data (pretending that we did not already know the parameters).  We will use Stan's defaults, which uses the multinomial no-U-turn sampler [@hoffman2014no,@betancourt2017conceptual] for fitting in four chains, with 1000 warmup and 1000 sampling iterations.

We will use the Python interface CmdStanPy [@stan2024csp].  Recall that we have defined the variable `data` above based on the simulation.  First, we define the model based on the Stan program.  Second, we sample from the posterior of that model given some data.  Third, we print a summary of the fit.  Before that, we import the cmdstanpy package and turn the logging level down to `ERROR` so that our output isn't cluttered with progress messages.

```{python python-code}
#| code-fold: true
import cmdstanpy as csp

import logging
csp.utils.get_logger().setLevel(logging.ERROR)
```

Once we've done that, fitting is a two-liner that first compiles the model, then samples given the data. 

```{python python-code}
#| code-fold: false

m = csp.CmdStanModel(stan_file="linear-regression.stan")
f = m.sample(data=data, show_progress=False)
```

Now that we have the fit, we can print a summary of the posterior.

```{python python-code}
#| code-fold: true
print(f.summary(sig_figs=3))
```

The first column lists all the variables of interest in the model, with `lp__` denoting the unnormalized log posterior density.  The remaining columns are defined per variable and include the posterior mean, Monte Carlo standard error, posterior standard deviation, mean absolute deviation (like standard deviation for medians), three quantiles (5\%, 50\%, and 95\%), as well as some convergence diagnostics.  The rank-normalized $\widehat{R}$ statistic is reported in the last column, and the effective sample size in the bulk and tail of the density, as well as bulk effective sample size per second (where we see this all ran quickly); definitions of all of these can be found in @vehtari2021rank.


# Stan's C++ model class transpilation target


```cpp
template <bool propto__, bool jacobian__, typename VecR>
inline auto log_prob_impl(VecR& params_r__) const {
  using T = stan::scalar_type_t<VecR>;
  T lp__(0.0);
  stan::math::accumulator<T> lp_accum__;
  stan::io::deserializer<T> in__(params_r__);
  auto alpha = in__.template read<T>();
  auto beta = in__.template read<Eigen::Matrix<T,-1,1>>(P);
  auto sigma = in__.template read_constrain_lb<T, jacobian__>(0, lp__);
  lp_accum__.add(stan::math::normal_lpdf<propto__>(alpha,
                    0, 5));
  lp_accum__.add(stan::math::normal_lpdf<propto__>(beta,
                    0, 2.5));
  lp_accum__.add(stan::math::exponential_lpdf<propto__>(sigma, 0.5));
  lp_accum__.add(stan::math::normal_lpdf<propto__>(y,
                    stan::math::add(alpha,
                      stan::math::multiply(beta, x)), sigma));
  lp_accum__.add(lp__);
  return lp_accum__.sum();
}
```




## Linear regression in JAX with `densejax`

The Stan linear regression can be translated almost line for line into Python using `densejax`.

```python
from densejax import (
    real, positive, normal, exponential, normal_rng, model
)

def linear_regression(x, y, x_new):
    N, P = x.shape
    
    parameters = {
      'a': real(),
      'b': real(size=P),
      's': positive()
    }                   

    def log_density(a, b, s):
        lp = 0
        lp += normal(a, 0, 2)
        lp += normal(b, 0, 1)
        lp += exponential(s, 0.5)
        lp += normal(y, a + x @ b, s)
        return lp

    def generate(rng, a, b, s):
        y_new = normal_rng(rng, a + x_new @ b, s)
        return { 'y_new': y_new }
        
    return model(parameters, log_density, generate)
```

The resulting `model` object mirrors the C++ object produced by Stan and also the model object produced by BridgeStan [@roualdes2023bridgestan].  Because these simple functions generate fully JAX-embedded code, the log density function can be automatically differentiated and all of the functions can be just-in-time compiled.

can compute log densities and their gradients, as well as transform parameters from the constrained to the unconstrained scale.  The parameter declarations with constraints, such as `positive`.  That allows it to be used for inference by `blackjax`, which has ported NUTS and ADVI from Stan into JAX and much much more.

In Python, the `arviz` package [@kumar2019arviz] provides extensive posterior analysis including means, standard deviations, quantiles, covariances, rank-normalized $\widehat{R}$-statistics for convergence monitoring, effective sample sizes, and estimator standard errors [@vehtari2021rank], as well as efficient approximate leave-one-out cross-validation for model comparison [@vehtari2017practical].


# References {.unnumbered}

::: {#refs}
:::
